{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "291965a8",
   "metadata": {},
   "source": [
    "Comare and evaluate how our different classifiers have done based on the test data not used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f3ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from extcats import CatalogQuery\n",
    "from scipy.stats import binned_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e7452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90399f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.TrainingValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a2d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e00d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = db.t2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0640f65c",
   "metadata": {},
   "source": [
    "Extract output from T2XgbClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f1620",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbdata = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4833accc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t2info in col.find({'unit':'T2XgbClassifier', \"code\":0}):\n",
    "    b = t2info['body'][-1]\n",
    "    b['stock'] = t2info['stock']\n",
    "    b['config'] = t2info['config']\n",
    "    b['code'] = t2info['code']\n",
    "    b['model'] = t2info['channel'][0]\n",
    "    b['link'] = t2info['link']\n",
    "    xgbdata.append( b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942d839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgb = pd.DataFrame.from_dict(xgbdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb3842",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad15bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tabdata = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b57f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t2info in col.find({'unit':'T2TabulatorRiseDecline', \"code\":0}):\n",
    "    # Only store required info\n",
    "    #b = t2info['body'][-1]\n",
    "    b = {}\n",
    "    b['stock'] = t2info['stock']\n",
    "    b['link'] = t2info['link']\n",
    "    b['ndet'] = t2info['body'][-1]['ndet']\n",
    "    b['success'] = t2info['body'][-1]['success']\n",
    "    tabdata.append( b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee7b694",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tab = pd.DataFrame.from_dict(tabdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tab.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a1fe67",
   "metadata": {},
   "source": [
    "Why did we retrieve the Tabulator data? Since the xgbdata output does not store (well) how many detections was used to construct the guess. In the end we will want to study how the classifications work as a function of detections. So we would wish to complement the xgbdata with the number of detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgbtab = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d24d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgbtab = pd.merge(df_xgb, df_tab, on='link', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e6cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgbtab['prob0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e50e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_xgbtab['ndet'], bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0118cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_xgbtab['config'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153634c8",
   "metadata": {},
   "source": [
    "How do we evaluate this? For a particular model (i.e. agn) and config we can define what the true result should be. So steps:\n",
    "- Figure out which config corresponds to which model. \n",
    "- For each of the configs, write a list of the models with which `is_0` is the True outcome. \n",
    "- Make some nice plots... We can play around with ndet, models, configs and prob0 so probably need to create some plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad47c330",
   "metadata": {},
   "source": [
    "From confid collection in TrainingValidation:\n",
    "5560207956784798794 xgb_v6_tree12_\n",
    "2508649458706045692 xgb_v6_tree121113_ \n",
    "-5649973164721472516 xgb_v6_tree2122_ \n",
    "8559538834424564852 xgb_v6_simmod_tree12_ \n",
    "2616828516668705481 xgb_v6_simmod_tree2122_ \n",
    "-6030627619870677701 xgb_v6_simmod_tree121113_\n",
    "As I recall their definition the first item would be the target for \"0\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7698048",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy = {\n",
    "    11:[\n",
    "        'sniasalt2',\n",
    " 'snictemplates',\n",
    " 'snibhostxtv19',\n",
    " 'snibtemplates',\n",
    " 'snichostxtv19',\n",
    " 'snicblhostxtv19',\n",
    " 'sniitemplates',\n",
    " 'sniibhostxtv19',\n",
    " 'sniinmosfit',\n",
    " 'sniihostxtv19',\n",
    " 'sniinmf',\n",
    " 'sniinhostxtv19',\n",
    " 'sniax',\n",
    " 'snia91bg',\n",
    "    ],\n",
    " 'snia':[\n",
    "    'sniasalt2',\n",
    "    'sniax',\n",
    "     'snia91bg',\n",
    "    ],\n",
    "  'sni':[\n",
    "        'sniasalt2',\n",
    " 'snictemplates',\n",
    " 'snibhostxtv19',\n",
    " 'snibtemplates',\n",
    " 'snichostxtv19',\n",
    " 'snicblhostxtv19',\n",
    " 'sniax',\n",
    " 'snia91bg',\n",
    "    ],\n",
    "  'snibc':[\n",
    " 'snictemplates',\n",
    " 'snibhostxtv19',\n",
    " 'snibtemplates',\n",
    " 'snichostxtv19',\n",
    " 'snicblhostxtv19',\n",
    "    ],\n",
    "  'snii':[\n",
    " 'sniitemplates',\n",
    " 'sniibhostxtv19',\n",
    " 'sniinmosfit',\n",
    " 'sniihostxtv19',\n",
    " 'sniinmf',\n",
    " 'sniinhostxtv19',\n",
    "    ],\n",
    "    12:[\n",
    " 'knk17',\n",
    " 'knb19',\n",
    " 'mdwarfflare',\n",
    " 'dwarfnova',\n",
    " 'ulenssinglegenlens',\n",
    " 'ulenssinglepylima',\n",
    " 'ulensbinary',            \n",
    "    ],\n",
    "    'ulens':[\n",
    " 'ulenssinglegenlens',\n",
    " 'ulenssinglepylima',\n",
    " 'ulensbinary',            \n",
    "    ],\n",
    "    'kn':[\n",
    " 'knk17',\n",
    " 'knb19',\n",
    "    ],    \n",
    "    13:[\n",
    " 'slsnihost',\n",
    " 'slsninohost',\n",
    " 'tde',\n",
    " 'ilot',\n",
    " 'cart',\n",
    " 'pisn',            \n",
    "    ],\n",
    "    'slsni':[\n",
    " 'slsnihost',\n",
    " 'slsninohost',\n",
    "    ],\n",
    "    21:[\n",
    " 'cepheid',\n",
    " 'rrl'\n",
    " 'dsct',\n",
    " 'eb',          \n",
    "    ],\n",
    "    22:[ 'agn',]           \n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e2fadd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelmaps = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4231b1",
   "metadata": {},
   "source": [
    "# xgb_v6_tree12_\n",
    "modelmaps[5560207956784798794] = {\n",
    "    'is0':[ 'sniasalt2',\n",
    " 'snictemplates',\n",
    " 'snibhostxtv19',\n",
    " 'snibtemplates',\n",
    " 'snichostxtv19',\n",
    " 'snicblhostxtv19',\n",
    " 'sniitemplates',\n",
    " 'sniibhostxtv19',\n",
    " 'sniinmosfit',\n",
    " 'sniihostxtv19',\n",
    " 'sniinmf',\n",
    " 'sniinhostxtv19',\n",
    " 'sniax',\n",
    " 'snia91bg',\n",
    " 'knk17',\n",
    " 'knb19',\n",
    " 'mdwarfflare',\n",
    " 'dwarfnova',\n",
    " 'ulenssinglegenlens',\n",
    " 'ulenssinglepylima',\n",
    " 'ulensbinary',\n",
    " 'slsnihost',\n",
    " 'slsninohost',\n",
    " 'tde',\n",
    " 'ilot',\n",
    " 'cart',\n",
    " 'pisn',],\n",
    "    'is1':[\n",
    "         'cepheid',\n",
    " 'rrl'\n",
    " 'dsct',\n",
    " 'eb',\n",
    " 'agn',\n",
    "]\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634fa3c3",
   "metadata": {},
   "source": [
    "# xgb_v6_tree121113_\n",
    "modelmaps[2508649458706045692] = {\n",
    "    'is0': [ 'knk17',\n",
    " 'knb19',\n",
    " 'mdwarfflare',\n",
    " 'dwarfnova',\n",
    " 'ulenssinglegenlens',\n",
    " 'ulenssinglepylima',\n",
    " 'ulensbinary',],\n",
    "    'is1': [\n",
    " 'sniasalt2',\n",
    " 'snictemplates',\n",
    " 'snibhostxtv19',\n",
    " 'snibtemplates',\n",
    " 'snichostxtv19',\n",
    " 'snicblhostxtv19',\n",
    " 'sniitemplates',\n",
    " 'sniibhostxtv19',\n",
    " 'sniinmosfit',\n",
    " 'sniihostxtv19',\n",
    " 'sniinmf',\n",
    " 'sniinhostxtv19',\n",
    " 'sniax',\n",
    " 'snia91bg', \n",
    " 'knk17',\n",
    " 'knb19',\n",
    " 'mdwarfflare',\n",
    " 'dwarfnova',\n",
    " 'ulenssinglegenlens',\n",
    " 'ulenssinglepylima',\n",
    " 'ulensbinary',        \n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee19840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_v6_tree2122_ \n",
    "modelmaps[-5649973164721472516] = {\n",
    "    'is0':[\n",
    "        'cepheid',\n",
    " 'rrl'\n",
    " 'dsct',\n",
    " 'eb',\n",
    "    ],\n",
    "    'is1': ['agn']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694af090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_v6_tree12_\n",
    "modelmaps[5560207956784798794] = {\n",
    "    'is0':[ 11, 12, 13],\n",
    "    'is1':[ 21, 22]\n",
    "}\n",
    "# xgb_v6_tree121113_\n",
    "modelmaps[2508649458706045692] = {\n",
    "    'is0':[ 12],\n",
    "    'is1':[ 11, 13]\n",
    "}\n",
    "# xgb_v6_tree2122_ \n",
    "modelmaps[-5649973164721472516] = {\n",
    "    'is0':[ 21],\n",
    "    'is1':[ 22]\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dee49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three others are variants of the first\n",
    "# xgb_v6_simmod_tree12_\n",
    "modelmaps[8559538834424564852] = modelmaps[5560207956784798794] \n",
    "# 2616828516668705481 xgb_v6_simmod_tree2122_\n",
    "modelmaps[2616828516668705481] = modelmaps[-5649973164721472516]\n",
    "# -6030627619870677701 xgb_v6_simmod_tree121113_\n",
    "#modelmaps[-6030627619870677701] = modelmaps[2508649458706045692]\n",
    "# or is this not reversed?\n",
    "modelmaps[-6030627619870677701] = {\n",
    "    'is0':[ 11, 13],\n",
    "    'is1':[ 12]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67969624",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edb7a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what we need\n",
    "\n",
    "def get_modellist(taxonomies):\n",
    "    '''\n",
    "    For a list of taxonomies, return a list of the models which should be included.\n",
    "    If the list is instead models, these will just be propagated.\n",
    "    '''\n",
    "    models = []\n",
    "    for tax in taxonomies:\n",
    "        if tax in taxonomy.keys():            \n",
    "            models.extend( taxonomy[tax] )\n",
    "        else:\n",
    "            models.append(tax)\n",
    "    return models\n",
    " \n",
    "\n",
    "def get_modelindex(models, mydf=df_xgbtab):\n",
    "    '''\n",
    "    For a list of models, return an index of rows correspondings to any of these. \n",
    "    '''\n",
    "    return mydf['model'].isin(models)\n",
    "    \n",
    "    \n",
    "def get_classifier_results(config, mydf=df_xgbtab, modelmaps=modelmaps):\n",
    "    '''\n",
    "    For a particular config, return a tuple\n",
    "    (ndet, ptrue, is1type)\n",
    "    where ndet is the number of detections and ptrue is the (possibly \n",
    "    inverted) provided probability that would have been correct. An ideal\n",
    "    classifier would thus have consistent one.\n",
    "    '''\n",
    "    \n",
    "    # 1. Find the subset relevant for this config\n",
    "    df_conf = mydf[mydf['config']==config]\n",
    "\n",
    "    # 2. Find rows which corresponds to H0 and H1 for this particular classifier.\n",
    "    iH0 = get_modelindex( get_modellist(modelmaps[config]['is0']), mydf=df_conf)\n",
    "    iH1 = get_modelindex( get_modellist(modelmaps[config]['is1']), mydf=df_conf)\n",
    "    \n",
    "    # 3. Process the H0 results\n",
    "    ndet_h0 = df_conf['ndet'][iH0]\n",
    "    # So this is the probability to be H0, which for these objects would be true.\n",
    "    # So prob0==correct\n",
    "    success_h0 = df_conf['prob0'][iH0]\n",
    "    type_h0 = np.zeros(len(success_h0))\n",
    "\n",
    "    # 4. Process the H1 results\n",
    "    ndet_h1 = df_conf['ndet'][iH1]\n",
    "    # So this is the probability to be H0, which for these objects would be false.\n",
    "    # So (1-prob0)==correct\n",
    "    success_h1 = 1.-df_conf['prob0'][iH1]\n",
    "    type_h1 = np.ones(len(success_h1))\n",
    "    \n",
    "    # 5. Return     \n",
    "    return pd.concat([ndet_h0,ndet_h1]), pd.concat([success_h0,success_h1]), np.append( type_h0, type_h1)\n",
    "\n",
    "\n",
    "def get_model_results(config, model, mydf=df_xgbtab, modelmaps=modelmaps):\n",
    "    '''\n",
    "    For a particular config and model, return a tuple\n",
    "    (ndet, ptrue)\n",
    "    where ndet is the number of detections and ptrue is the (possibly \n",
    "    inverted) provided probability that would have been correct. \n",
    "    '''\n",
    "    \n",
    "    # 1. Find the subset relevant for this config\n",
    "    df_conf = mydf[mydf['config']==config]\n",
    "\n",
    "    # 2. Find whether this model corresponds to the H0 or H1 predictions (or none)\n",
    "    if model in get_modellist(modelmaps[config]['is0']):\n",
    "        print('H0 model')\n",
    "        h1 = False\n",
    "    elif model in get_modellist(modelmaps[config]['is1']):\n",
    "        print('H1 model')\n",
    "        h1 = True\n",
    "    else:\n",
    "        print('Classifier not trained for this model')\n",
    "        return None, None\n",
    "    # Get model inidces\n",
    "    iModel = get_modelindex( [model], mydf=df_conf)\n",
    "    \n",
    "    \n",
    "    # 4. Process the H1 results\n",
    "    ndet = df_conf['ndet'][iModel]\n",
    "    # So this is the probability to be H0, which for these objects would be false.\n",
    "    # So (1-prob0)==correct\n",
    "    if h1:\n",
    "        success = 1.-df_conf['prob0'][iModel]\n",
    "    else:\n",
    "        success = df_conf['prob0'][iModel]\n",
    "    \n",
    "    # 5. Return     \n",
    "    return ndet, success    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb64b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "det, suc = get_model_results(2508649458706045692, 'knk17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e936534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "det, suc, typ  = get_classifier_results(8559538834424564852)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(det, suc, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8eb35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed ndet cuts \n",
    "ndet_cuts = [-1.0e-03,  1.0e+00,  2.0e+00,  4.0e+00,  5.0e+00,\n",
    "        8.0e+00,  1.1e+01, 1.6e+01,  1.9e+01,  2.2e+01,  2.6e+01,\n",
    "        3.0e+01,  3.6e+01,  4.5e+01,  6.3e+01, 10**3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e86bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fep = pd.qcut(det, 30, duplicates='drop')\n",
    "fep = pd.cut(det, ndet_cuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6112bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "suc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ff6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "suc2 = pd.DataFrame({'suc':suc, 'qdet':fep, 'ndet':det})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f20bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "confmean = suc2.groupby(['qdet']).mean()\n",
    "confstd = suc2.groupby(['qdet']).std()\n",
    "confcount = suc2.groupby(['qdet']).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a8b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "confmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bba420",
   "metadata": {},
   "outputs": [],
   "source": [
    "suc2['qdet'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b28da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndet_bins = np.unique([ (v.right-v.left)/2+v.left for v in suc2['qdet'].values] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26223a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(confmean['ndet'], confmean['suc'], yerr=confstd['suc']/np.sqrt(confcount['suc']), fmt='.')\n",
    "plt.plot(confmean['ndet'], confmean['suc'], 'o', ms=12, color='dodgerblue')\n",
    "plt.xlabel('Number of (significant) detection')\n",
    "plt.ylabel('Prob of classification being correct')\n",
    "plt.savefig('/home/jnordin/tmp/modeleval.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db1233",
   "metadata": {},
   "source": [
    "With the above we can study some things:\n",
    "- For each individual model, how well do the different classifiers work (as a function of number of detectoins).\n",
    "- For each taxonomy group, how well do the classifiers work?\n",
    "- Can we improve things through changing e.g. range of detections?\n",
    "\n",
    "Next sep:\n",
    "- Make some nice summary plot that in one figure compares how the different classifiers work.\n",
    "- For each classifier, draw lines for all of the different models to study whether some work particularly bad. \n",
    "- Look at parsnip, possible under the constraint of using the fine-grained tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5342a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for config, group in df_xgb.groupby(['model','config']):\n",
    "    print('Model {}'.format(group['model'].iloc[0]))\n",
    "    print('Config {} for {} fits out of which {} failed.'.format(config, group.shape[0], sum(group['xgbsuccess']==False)))\n",
    "    sub = group[ ~(group['xgbsuccess']==False) ]\n",
    "    print('Out of the remaining {}, {} are classified as class O ({})'.format(\n",
    "                        sub.shape[0], sum(group['is_0']==True), sum(group['is_0']==True)/sub.shape[0]))\n",
    "    plt.figure()\n",
    "#    plt.title(sub['model'][0])\n",
    "    plt.hist(sub['imodel'])\n",
    "    plt.figure()\n",
    "#    plt.title(sub['model'][0])\n",
    "    plt.hist(sub['prob0'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e776f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsnipdata = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fdaf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we are evaluating the different t2runparsnip runs we have\n",
    "# 2244063382525234836   -     old config (w/o ulens)\n",
    "# 9048316057906136269   -     new config (w/o SLSN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t2info in col.find({'unit':'T2RunParsnip', \"code\":0}):\n",
    "    try:\n",
    "        classes = t2info['body'][0]['classification']\n",
    "        classes['dof'] = t2info['body'][0]['prediction']['model_dof']\n",
    "        classes['chisq'] = t2info['body'][0]['prediction']['model_chisq']\n",
    "    except KeyError:\n",
    "        classes = {}\n",
    "    classes['model'] = t2info['channel'][0]\n",
    "    classes['stock'] = t2info['stock']\n",
    "    classes['link'] = t2info['link']    \n",
    "    classes['config'] = t2info['config']    \n",
    "    parsnipdata.append( classes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca549d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsnip = pd.DataFrame.from_dict(parsnipdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a0271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update that column names to match our taxonomy\n",
    "parsnip_colmap = {'CART':'cart', 'ILOT':'ilot', 'KN':'kn', 'Mdwarf-flare':'mdwarfflare', \n",
    "                  'PISN':'pisn', 'SLSN-I':'slsni', 'SNII':'snii', 'SNIa':'sniasalt2',\n",
    "             'SNIa91bg':'snia91bg', 'SNIax':'sniax', 'SNibc':'snibc', 'TDE':'tde', \n",
    "                  'dwarf-nova':'dwarfnova', 'uLens':'ulens'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsnip.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50afa16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsnip['model'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for oname, newname in parsnip_colmap.items():\n",
    "df_parsnip.rename(parsnip_colmap, axis=1, inplace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf925ab",
   "metadata": {},
   "source": [
    "How do we wish to evaluate parsnip? Here we have more moving parts in terms of specific models, dof and chi2, as well as whether we consider different subsets. \n",
    "So we need to take into account:\n",
    "- That some taxonomy groups could have been counted as removed (or not).\n",
    "- That we could have some limits to dof and chi.\n",
    "- Plot both the fraction of correctly classified objects as well as well as other kind of objects classified as this. \n",
    "\n",
    "When we have two separate parsnip runs, who should these be evaluated? For first stage it is probably easiest to create two files and then switch which df_parsnip points to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3811acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsnipm1 = df_parsnip[df_parsnip['config']==2244063382525234836]\n",
    "df_parsnipm2 = df_parsnip[df_parsnip['config']==9048316057906136269]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2df680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply reimplement these for the parsnip datafiles?\n",
    "\n",
    " \n",
    "\n",
    "def parsnip_modelcolumns(models, mydf=df_parsnip):\n",
    "    '''\n",
    "    For a parsnip datafile, return the subset of columns correspond to the listed models.\n",
    "    Addid for completeness\n",
    "    '''\n",
    "    return mydf[models]\n",
    "    \n",
    "\n",
    "def parsnip_models_results(pred_models, true_models, marginalize_models = [], mydf=df_parsnip):\n",
    "    '''\n",
    "    For a list of model, return a tuple\n",
    "    (ndet, chisq, pvalue, is_model)\n",
    "    where ndet is the number of detections, chisq the fit chisq value \n",
    "    and pvalue to _summed_ probability values \n",
    "    (summed if results from multiple models are used)\n",
    "    and is_model a boolean describing whether the fitted model is the expected.\n",
    "    \n",
    "    marginalize_models correspond to a list of column (predicted models) which should \n",
    "    first be removed and the remaining probabilities rescaled.\n",
    "    '''\n",
    "    \n",
    "    # 0. Marginalize\n",
    "    if len(marginalize_models)>0:\n",
    "        df_marge = parsnip_modelcolumns(marginalize_models, mydf=mydf)\n",
    "        marge_prob = 1. / ( 1.-df_marge.sum(axis=1) )\n",
    "    else:\n",
    "        marge_prob = 1.\n",
    "    \n",
    "    # 1. Get the probabilities \n",
    "    # Here we should not use the taxonomy expansion, we are assuming that \n",
    "    # we are using columns from parsnip_colmap\n",
    "    df_pred = parsnip_modelcolumns( pred_models, mydf=mydf)    \n",
    "    prob = df_pred.sum(axis=1)\n",
    "    prob *= marge_prob\n",
    "\n",
    "    # 2. Find which rows were generated based on the list of models.\n",
    "    iModel = get_modelindex( get_modellist(true_models), mydf=mydf)\n",
    "    is_model = np.zeros(len(prob),dtype=bool)\n",
    "    is_model[iModel] = True\n",
    "    \n",
    "    return mydf['dof'], mydf['chisq'], prob, is_model\n",
    "    \n",
    "    \n",
    "def parsnip_cut_models(cut_models, mydf=df_parsnip, modelmaps=modelmaps):\n",
    "    '''\n",
    "    Return a version of the datafiles where a number of models have been cut\n",
    "    *and* the remaining model columns rescaled such that probabilities add up \n",
    "    to one.\n",
    "    '''\n",
    "    \n",
    "    # 1. Get the probabilities of the cut columns\n",
    "    df_pred = parsnip_modelcolumns(pred_models, mydf=mydf)\n",
    "    cut_prob = df_pred.sum(axis=1)\n",
    "    \n",
    "    # 1. Find the subset relevant for this config\n",
    "    df_conf = mydf[mydf['config']==config]\n",
    "\n",
    "    # 2. Find rows which corresponds to H0 and H1 for this particular classifier.\n",
    "    iH0 = get_modelindex( get_modellist(modelmaps[config]['is0']), mydf=df_conf)\n",
    "    iH1 = get_modelindex( get_modellist(modelmaps[config]['is1']), mydf=df_conf)\n",
    "    \n",
    "    # 3. Process the H0 results\n",
    "    ndet_h0 = df_conf['ndet'][iH0]\n",
    "    # So this is the probability to be H0, which for these objects would be true.\n",
    "    # So prob0==correct\n",
    "    success_h0 = df_conf['prob0'][iH0]\n",
    "    type_h0 = np.zeros(len(success_h0))\n",
    "\n",
    "    # 4. Process the H1 results\n",
    "    ndet_h1 = df_conf['ndet'][iH1]\n",
    "    # So this is the probability to be H0, which for these objects would be false.\n",
    "    # So (1-prob0)==correct\n",
    "    success_h1 = 1.-df_conf['prob0'][iH1]\n",
    "    type_h1 = np.ones(len(success_h1))\n",
    "    \n",
    "    # 5. Return     \n",
    "    return pd.concat([ndet_h0,ndet_h1]), pd.concat([success_h0,success_h1]), np.append( type_h0, type_h1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dof, chisq, prob, is_model = parsnip_models_results(['ulens'], ['ulens'], mydf=df_parsnipm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ec84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dof[is_model], prob[is_model], 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd96dc4e",
   "metadata": {},
   "source": [
    "Time to return to the question of what it is we want to find out.\n",
    "- Is parsnip working good enough, i.e. at all as good as we expect?\n",
    "- Would it be meaningfully improved by limiting to some taxonomies (i.e. trusting xgb further)\n",
    "- Would it improve to include some min requirements on dof and/or chisq/dof?\n",
    "- How much of the \"bad\" behaviour is caused by similar models being wrongly classified, i.e. things you could accept?\n",
    "\n",
    "The basic plot we would like to see is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a48b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuts for plot\n",
    "min_dof = 0\n",
    "min_chisqdof = 0\n",
    "max_chisqdof = 99\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a1edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parsnip_cutbins(dof, chisq, prob, \n",
    "                        min_dof=0, min_chisqdof=0, max_chisqdof=999, xcol='dof',bins=10):\n",
    "    \"\"\"\n",
    "    Return version of data results which have been cut and then binned.\n",
    "    \n",
    "    Bins can either be a number of bins, or a spcific set of bins.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Do cuts\n",
    "    chisqdof = chisq/dof\n",
    "    iCut = ( (dof>=min_dof) & (chisqdof>=min_chisqdof)  & (chisqdof<=max_chisqdof))\n",
    "    \n",
    "    if xcol=='dof':\n",
    "        xval = dof\n",
    "    elif xcol=='chisqdof':\n",
    "        xval = chisqdof\n",
    "    \n",
    "    # Binning\n",
    "    if isinstance(bins, int):\n",
    "        bingroup = pd.qcut(xval[iCut], bins, duplicates='drop')\n",
    "    else:\n",
    "        bringroup = pd.cut(xval[iCut], bins)\n",
    "    \n",
    "    # Bin and do simple stats\n",
    "    fp_tmp = pd.DataFrame({'prob':prob[iCut], 'ibin':bingroup, 'xval':xval[iCut]})    \n",
    "    confmean = fp_tmp.groupby(['ibin']).mean()\n",
    "    confstd = fp_tmp.groupby(['ibin']).std()\n",
    "    confcount = fp_tmp.groupby(['ibin']).count()\n",
    "#    print(confmean)\n",
    "    \n",
    "    return confmean['xval'], confmean['prob'], confstd['prob']/np.sqrt(confcount['prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa34ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "#plt.title(model)\n",
    "    \n",
    "for model in parsnip_colmap.values():\n",
    "    \n",
    "    # This is either a single model, or a taxonomy composite\n",
    "    \n",
    "    print(model)\n",
    "#    models = get_modellist([model])\n",
    "#    print('translated into models')\n",
    "\n",
    "    # Check predictions of being this model\n",
    "    dof, chisq, prob, is_model = parsnip_models_results([model], [model], mydf=df_parsnipm1)\n",
    "    # The probability \n",
    "    # dof[is_model] ...\n",
    "    \n",
    "    # Bin according to number of detections and plot\n",
    "    pos, prob, prob_err = get_parsnip_cutbins(dof[is_model], chisq[is_model], prob[is_model],\n",
    "                                             xcol='chisqdof')\n",
    "    \n",
    "    plt.errorbar(pos, prob, yerr=prob_err, fmt='.', color='black')\n",
    "#    plt.plot(pos,prob, 'o', ms=12, color='dodgerblue')\n",
    "    plt.plot(pos,prob, 'o', ms=12, label=model)\n",
    "plt.legend()\n",
    "plt.xlabel('Chisq / d.o.f.')\n",
    "plt.ylabel('Prob of classification being correct')\n",
    "plt.xlim(0,5)\n",
    "plt.show()\n",
    "# Conclusion: there is no enormous improvement to justify cutting on d.o.f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02542d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "#plt.title(model)\n",
    "    \n",
    "for model in parsnip_colmap.values():\n",
    "    \n",
    "    # This is either a single model, or a taxonomy composite\n",
    "    \n",
    "    print(model)\n",
    "#    models = get_modellist([model])\n",
    "#    print('translated into models')\n",
    "\n",
    "    # Check predictions of being this model\n",
    "    dof, chisq, prob, is_model = parsnip_models_results([model], [model], mydf=df_parsnipm2)\n",
    "    # The probability \n",
    "    # dof[is_model] ...\n",
    "    \n",
    "    # Bin according to number of detections and plot\n",
    "    pos, prob, prob_err = get_parsnip_cutbins(dof[is_model], chisq[is_model], prob[is_model],\n",
    "                                             xcol='dof')\n",
    "    \n",
    "    plt.errorbar(pos, prob, yerr=prob_err, fmt='.', color='black')\n",
    "#    plt.plot(pos,prob, 'o', ms=12, color='dodgerblue')\n",
    "    plt.plot(pos,prob, 'o', ms=12, label=model)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of degrees of freedom')\n",
    "plt.ylabel('Prob of classification being correct')\n",
    "#plt.xlim(0,20)\n",
    "plt.show()\n",
    "# Conclusion: Similarly, there is no gigantic improvement with more datapoint.\n",
    "# it does get better, but not drasticly so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85648f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What models should we marginalize over (remove)?\n",
    "print(df_parsnip.columns)\n",
    "print(set(df_parsnip['model']))\n",
    "remove_models = ['kn', 'cart', 'ilot', 'mdwarfflare', 'dwarfnova', 'pisn', 'tde']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now redo the above for the most interesting groups\n",
    "taxgroups = ['snia', 'snii', 'snibc', 'slsni']\n",
    "parsnip_taxnames = {'snii':['snii'], 'snibc':['snibc'], 'slsni':['slsni']}\n",
    "for taxgroup in taxgroups:\n",
    "    simmodels = get_modellist([taxgroup])\n",
    "    print(simmodels)\n",
    "    if taxgroup in parsnip_taxnames.keys():\n",
    "        parsnip_models = parsnip_taxnames[taxgroup]\n",
    "    else:\n",
    "        parsnip_models = simmodels\n",
    "    print(taxgroup, simmodels, parsnip_models)\n",
    "    # Get results\n",
    "    dof, chisq, prob, is_model = parsnip_models_results(parsnip_models, simmodels, \n",
    "                                                        remove_models, mydf=df_parsnipm2)\n",
    "    # Bin according to number of detections and plot\n",
    "    pos, prob, prob_err = get_parsnip_cutbins(dof[is_model], chisq[is_model], prob[is_model],\n",
    "                                             xcol='dof')\n",
    "    \n",
    "    plt.errorbar(pos, prob, yerr=prob_err, fmt='.', color='black')\n",
    "    plt.plot(pos,prob, 'o', ms=12, label=taxgroup)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of degrees of freedom')\n",
    "plt.ylabel('Prob of classification being correct')\n",
    "#plt.xlim(0,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb37580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now redo the above for the most interesting groups\n",
    "taxgroups = ['snia', 'snii', 'snibc', 'slsni']\n",
    "parsnip_taxnames = {'snii':['snii'], 'snibc':['snibc'], 'slsni':['slsni']}\n",
    "for taxgroup in taxgroups:\n",
    "    simmodels = get_modellist([taxgroup])\n",
    "    print(simmodels)\n",
    "    if taxgroup in parsnip_taxnames.keys():\n",
    "        parsnip_models = parsnip_taxnames[taxgroup]\n",
    "    else:\n",
    "        parsnip_models = simmodels\n",
    "    # Get results\n",
    "    dof, chisq, prob, is_model = parsnip_models_results(parsnip_models, simmodels, \n",
    "                                                        mydf=df_parsnipm1)\n",
    "    # Bin according to number of detections and plot\n",
    "    pos, prob, prob_err = get_parsnip_cutbins(dof[is_model], chisq[is_model], prob[is_model],\n",
    "                                             xcol='chisqdof')\n",
    "    \n",
    "    plt.errorbar(pos, prob, yerr=prob_err, fmt='.', color='black')\n",
    "    plt.plot(pos,prob, 'o', ms=12, label=taxgroup)\n",
    "plt.legend()\n",
    "plt.xlabel('Chi / dof')\n",
    "plt.ylabel('Prob of classification being correct')\n",
    "plt.xlim(0,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results are so bad, could we have mixed up columns? Look at some models\n",
    "simmodel = 'ulens'\n",
    "# prolematics:\n",
    "# sniibhostxtv19\n",
    "# slsninohost\n",
    "# ulens*\n",
    "pmodels = ['cart', 'ilot', 'kn', 'mdwarfflare', 'pisn', 'slsni', 'snii',\n",
    "       'sniasalt2', 'snia91bg', 'sniax', 'snibc', 'tde', 'dwarfnova']\n",
    "#pmod = [pmodels[12]]\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "for pmod in pmodels:\n",
    "    dof, chisq, prob, is_model = parsnip_models_results([pmod], [simmodel], mydf=df_parsnipm2)\n",
    "    # Bin according to number of detections and plot\n",
    "    pos, prob, prob_err = get_parsnip_cutbins(dof[is_model], chisq[is_model], prob[is_model],\n",
    "                                             xcol='dof')\n",
    "    \n",
    "    plt.errorbar(pos, prob, yerr=prob_err, fmt='.', color='black')\n",
    "    plt.plot(pos,prob, 'o', ms=12, label=pmod)\n",
    "plt.legend()\n",
    "plt.title('Simulation of model '+simmodel)\n",
    "plt.xlabel('Number of degrees of freedom')\n",
    "plt.ylabel('Prob to be classified as')\n",
    "#plt.xlim(0,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81baf919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which models should we look at?\n",
    "#pmodels = ['snii', 'snibc', 'sniasalt2', 'snia91bg', 'sniax', \n",
    "#    'cart', 'ilot', 'pisn', 'slsni', 'tde', 'kn', 'mdwarfflare', 'dwarfnova']\n",
    "#pmodels = ['snii', 'snibc', 'sniasalt2', 'snia91bg', 'sniax', \n",
    "#    'cart', 'ilot', 'pisn', 'slsni', 'tde']\n",
    "#pmodels = ['snii', 'snibc', 'sniasalt2', 'snia91bg', 'sniax', ]\n",
    "pmodels = ['kn', 'mdwarfflare', 'dwarfnova', 'ulens']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55408ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parcut = df_parsnipm2.copy(deep=True)\n",
    "#df_parcut = df_parsnipm2.copy(deep=True)\n",
    "# Now, let us look at the most likely classification for each row\n",
    "df_parcut['pmax'] = df_parsnipm2[pmodels].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b067c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parcut['pmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029833bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next question will be to map the simulation models to the pmax values\n",
    "# these are the mappings we will need to do (the rest we can leave )\n",
    "modmap = {'knb19':'kn', 'knk17':'kn', 'slsnihost':'slsni', 'slsninohost':'slsni', \n",
    "         'snibhostxtv19':'snibc', 'snibtemplates':'snibc', 'snicblhostxtv19':'snibc',\n",
    "         'snichostxtv19':'snibc', 'snictemplates':'snibc', 'sniibhostxtv19':'snii',\n",
    "         'sniihostxtv19':'snii', 'sniinhostxtv19':'snii', 'sniinmf': 'snii', 'sniinmosfit':'snii',\n",
    "         'sniitemplates':'snii', 'ulenssinglegenlens':'ulens', 'ulenssinglepylima':'ulens', \n",
    "          'ulensbinary':'ulens'}\n",
    "for mod in list(set(df_parcut['pmax'])):\n",
    "    modmap[mod] = mod\n",
    "\n",
    "# Remove some particularly bad models\n",
    "#modmap.pop('slsninohost')\n",
    "#modmap.pop('sniibhostxtv19')\n",
    "\n",
    "print(modmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10639f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parcut['model_parsnipconf'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976e6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parcut['model_parsnipconf'] = df_parcut['model'].map(modmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614521c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae27e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which model and in what order to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09c571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We either use all models that we included in pmodels, or all...\n",
    "#iMatrix = (~df_parcut['model_parsnipconf'].isna()) & (~df_parcut['pmax'].isna()) & df_parcut['model_parsnipconf'].isin(pmodels)\n",
    "iMatrix = (~df_parcut['model_parsnipconf'].isna()) & (~df_parcut['pmax'].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b2b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = list( set(df_parsnip['pmax']) )[1:] # skip nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(df_parcut['model_parsnipconf'][iMatrix], df_parcut['pmax'][iMatrix], labels=pmodels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = cm.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm/np.tile(norm, len(pmodels)).reshape([len(pmodels),len(pmodels)]).transpose(), display_labels=pmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f20e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "disp.plot(ax=ax)\n",
    "#ax.show()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3641143",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "disp.plot(ax=ax)\n",
    "#ax.show()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a2a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First plot, plot the different classifications for some subset with a min dof\n",
    "df_parsnip_dof = df_parsnipm2[df_parsnipm2['dof']>5]\n",
    "df_pardof = df_parsnip_dof.drop(['dof', 'stock'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ef51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, group in df_parsnip_dof.groupby(['model']):\n",
    "    run_ok = sum(group['dof']>0)\n",
    "    print('Model {} for {} fits out of which {} worked.'.format(model, group.shape[0], run_ok))\n",
    "#    sub = group[ ~(group['xgbsuccess']==False) ]\n",
    "#    print('Out of the remaining {}, {} are classified as class O ({})'.format(\n",
    "#                        sub.shape[0], sum(group['is_0']==True), sum(group['is_0']==True)/sub.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43bce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "parnsip_classmean = df_pardof.groupby(['model']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc03115",
   "metadata": {},
   "outputs": [],
   "source": [
    "parnsip_classstd = df_pardof.groupby(['model']).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27fa528",
   "metadata": {},
   "outputs": [],
   "source": [
    "parnsip_classmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1835ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot = parnsip_classmean.loc['agn'].plot(kind='bar',yerr=parnsip_classstd.loc['agn'],colormap='OrRd_r',edgecolor='black',grid=False,figsize=(8,2),ax=ax,position=0.45,error_kw=dict(ecolor='black',elinewidth=0.5),width=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9df444",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Next kind of plot, for a given run model (e.g. agn), plot how the fractions of classifications evolve with dof.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ca9baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model = 'sniasalt2'\n",
    "comp_classes = ['PISN', 'SLSN-I', 'SNII', 'SNIa', 'SNIa91bg', 'SNIax', 'SNibc']\n",
    "doflimits = list( range(0,85,5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classevo(df_model, compclass, doflimits):\n",
    "    \"\"\"\n",
    "    Get the classification for entries within dof bins\n",
    "    \"\"\"\n",
    "    means, binds, binnbr = binned_statistic(df_model['dof'], df_model[compclass], bins=doflimits)\n",
    "    return means\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e92b24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30705d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(doflimits[1:], means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2944af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_salt_ampelz.columns)\n",
    "print(df_z.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798b09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_salt_ampelz.duplicated('stock'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c3b06",
   "metadata": {},
   "source": [
    "We now want to merge the ampelz list with the salt output. The first should be larger than the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a386de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ampelz = df_z.merge(df_salt_ampelz, how='outer', on='stock', suffixes=('_ampelz',None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3540b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ampelz[ df_ampelz.duplicated('stock') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb4756c",
   "metadata": {},
   "source": [
    "3. We now redo this for results retrieved from BTS. Here we do not need DigestRedshift results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4cc42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "salt_btsz_plots = get_saved_saltfits(7231927385063365296, 4)\n",
    "salt_btsz_plots = pd.DataFrame.from_dict(salt_btsz_plots)\n",
    "salt_btsz_plots.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c2904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "salt_btsz_noplots = get_saved_saltfits(-5764621775949025619, 4)\n",
    "salt_btsz_noplots = pd.DataFrame.from_dict(salt_btsz_noplots)\n",
    "salt_btsz_noplots.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cef122",
   "metadata": {},
   "outputs": [],
   "source": [
    "salt_btsz = salt_btsz_noplots.append( salt_btsz_plots )\n",
    "salt_btsz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step will beto go through the above and for events where muliple rows exist, show \"the best\"\n",
    "# to retain. Or, actually we do not need to? We can plot all events and the selection will\n",
    "# naturally be part of how we cut things down. Will only have to think of how we combine the tables\n",
    "# and how we do the accounting after cutting.\n",
    "len(salt_btsz['stock'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0223d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "btsz_unique = []\n",
    "for stock in salt_btsz['stock'].unique():\n",
    "    stock_sub = salt_btsz[salt_btsz['stock']==stock]\n",
    "    \n",
    "    if stock_sub.shape[0]==1:\n",
    "        btsz_unique.append( dict(stock_sub.iloc[0]) )\n",
    "        continue\n",
    "\n",
    "               \n",
    "    # As a first step, we can choose the subset with most dof around peak\n",
    "    stock_sub = stock_sub[ stock_sub['nbr_peak_pulls']==stock_sub['nbr_peak_pulls'].max() ]\n",
    "    if stock_sub.shape[0]==1:\n",
    "        btsz_unique.append( dict(stock_sub.iloc[0]) )\n",
    "        continue\n",
    "        \n",
    "    # If some have very few ndof we can use this\n",
    "    if stock_sub['ndof'].max()>15 and stock_sub['ndof'].min()<15:\n",
    "        stock_sub = stock_sub[ stock_sub['ndof']>15 ]\n",
    "    \n",
    "    \n",
    "    # We now take the smallest chisq / ndof\n",
    "    chisqdof = stock_sub['chisq'] / stock_sub['ndof']\n",
    "    stock_sub = stock_sub[chisqdof==chisqdof.min()]\n",
    "\n",
    "    # Just test this\n",
    "    btsz_unique.append( dict(stock_sub.iloc[0]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(btsz_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb835d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salt_btsz = pd.DataFrame.from_dict(btsz_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7410e906",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salt_btsz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c310d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done already?\n",
    "df_btsz = df_salt_btsz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_btsz.duplicated('stock'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b44e47d",
   "metadata": {},
   "source": [
    "3. Next we will look for host information from the Zou et al LS catalog, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "hostinfo = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78972eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t2info in col.find({\"unit\":\"T2CatalogMatch\", \"body.LSPhotoZZou.dist2transient\":{\"$exists\": True} }):\n",
    "    b = t2info['body'][-1]['LSPhotoZZou']\n",
    "    store = { 'Zou_'+k:float(v) for k, v in b.items() if k in \n",
    "             ['dist2transient','photoz','e_photoz','_6','logMassBest','logMassInf','logMassSup'] }\n",
    "    store['stock'] = t2info['stock']\n",
    "    hostinfo.append(store)\n",
    "    if len(hostinfo)%1000==0:\n",
    "        print(len(hostinfo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e1a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_host = pd.DataFrame.from_dict(hostinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f6561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_host.shape)\n",
    "df_host = df_host.drop_duplicates()\n",
    "print(df_host.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, differences are small, can take the last\n",
    "for k, stock in enumerate( df_host['stock'][df_host.duplicated('stock')].unique() ):\n",
    "    df_subset = df_host[df_host['stock']==stock]\n",
    "    print(df_subset)\n",
    "    if k>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e704a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_host = df_host.drop_duplicates('stock', keep='last')\n",
    "print(df_host.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c736501",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_host.duplicated('stock'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a520743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ampelz = df_ampelz.merge(df_host, how='outer', on='stock', suffixes=(None,None))\n",
    "df_btsz = df_btsz.merge(df_host, how='left', on='stock', suffixes=(None,None))  # No point in adding all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c85099",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_ampelz.duplicated('stock'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf0efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_btsz.duplicated('stock'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203e2592",
   "metadata": {},
   "source": [
    "2. Extract BTS information if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f76e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "btsinfo = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0680b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t2info in col.find({\"body.bts_peakmag\":{\"$exists\": True} }):\n",
    "    b = t2info['body'][-1]\n",
    "    store = { k:v for k, v in b.items() if k in \n",
    "             ['bts_peakfilt','bts_duration','bts_type'] }\n",
    "    store['stock'] = t2info['stock']\n",
    "    \n",
    "    for p in ['bts_peakt', 'bts_peakmag', 'bts_redshift']:\n",
    "        try:\n",
    "            store[p] = float(b[p])\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    btsinfo.append(store)\n",
    "    if len(btsinfo)%1000==0:\n",
    "        print(len(btsinfo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9085cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bts = pd.DataFrame.from_dict(btsinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bts = df_bts.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb992c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(df_bts['bts_redshift'],bins=100)\n",
    "#plt.xlim([0,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed08df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_bts['bts_type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e58052",
   "metadata": {},
   "source": [
    "As target BTS type we will consider ['SN Ia', 'SN Ia-91T'], as transient other ['SN IIb', 'SN II', 'SN Ib', 'SN IIP', SN Ic-BL', 'SN Ia-CSM', 'SN Ia-pec', 'SLSN-I', 'SN IIn', 'SN Ic', 'SN Ia-CSM', 'SN Ib/c', SLSN-II', 'SN Iax', 'SN II-pec', 'TDE', 'CV', 'SN Ibn', 'SN Ib-pec', 'SN Ia-91bg', 'SN Ia-SC', 'SN Icsn'] and static ['AGN', 'LBV', 'ILRT']. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56bc91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum( df_bts['bts_type'].isin(['SN Ia', 'SN Ia-91T']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdbbe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a new entry, isIa, which is set to 1 if this is known SNIa, as -1 if known other transient and -2 if other types.\n",
    "df_bts['isIa'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8539a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bts.loc[ list(df_bts['bts_type'].isin(['SN Ia', 'SN Ia-91T'])), 'isIa' ] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc89f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bts.loc[ list(df_bts['bts_type'].isin(['SN IIb', 'SN II', 'SN Ib', 'SN IIP', 'SN Ic-BL', 'SN Ia-CSM', 'SN Ia-pec', 'SLSN-I', 'SN IIn', 'SN Ic', 'SN Ia-CSM', 'SN Ib/c', 'SLSN-II', 'SN Iax', 'SN II-pec', 'TDE', 'CV', 'SN Ibn', 'SN Ib-pec', 'SN Ia-91bg', 'SN Ia-SC', 'SN Icsn'])), 'isIa' ] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f563c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bts.loc[ list(df_bts['bts_type'].isin(['AGN', 'LBV', 'ILRT'])), 'isIa' ] = -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bc1b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bts['isIa'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f5a359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e043fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We find %s bts Z entries from %s unique transients.'%(len(df_bts['stock']),len(df_bts['stock'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e97d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us try to merge these were possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b8e109",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ampelz = df_ampelz.merge(df_bts, how='outer', on='stock', suffixes=(None,None)) # This is where some duplicates enter?\n",
    "df_btsz = df_btsz.merge(df_bts, how='outer', on='stock', suffixes=(None,None))  # No point in adding all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd256ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ampelz['z_dist'].hist()\n",
    "df_ampelz['z'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc7fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.cosmology import Planck18\n",
    "import numpy as np\n",
    "from astropy import units as U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06226d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ampelz['ang_dist_kpc'] = Planck18.angular_diameter_distance(df_ampelz['z']).values / U.Mpc / ( 360 * 60 * 60 / (2*np.pi) ) * df_ampelz['z_dist'] * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Planck18.angular_diameter_distance(0.08) / ( 360 * 60 * 60 / (2*np.pi) ) / U.Mpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For bts we will use the Zou data, since we did not add the DigestRedshift stuff. Probably should have?\n",
    "df_btsz['ang_dist_kpc'] = Planck18.angular_diameter_distance(df_btsz['z']).values / U.Mpc / ( 360 * 60 * 60 / (2*np.pi) ) * df_btsz['Zou_dist2transient'] * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd204e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_btsz['ang_dist_kpc'],bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad9d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ampelz[ df_ampelz['stock']==304843764 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c303ea",
   "metadata": {},
   "source": [
    "Let us have a look at parsnip. Parsnip was only run based on the ampelz data, but we can in principle add them also to the bts sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914e5739",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn = get_saved_parsnip(1079222371588722561, 4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eafbc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsnip = pd.DataFrame.from_dict(pn)\n",
    "print(df_parsnip.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ff96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove complete duplicates\n",
    "df_parsnip = df_parsnip.drop_duplicates()\n",
    "print(df_parsnip.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b94e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have to look at duplicate states\n",
    "parsnip_unique = []\n",
    "for k, stock in enumerate(df_parsnip['stock'].unique()):\n",
    "    stock_sub = df_parsnip[df_parsnip['stock']==stock]\n",
    "    \n",
    "    if stock_sub.shape[0]==1:\n",
    "        parsnip_unique.append( dict(stock_sub.iloc[0]) )\n",
    "        continue\n",
    "\n",
    "    # If some have very few ndof we can remove these\n",
    "    if stock_sub['model_dof'].max()>15 and stock_sub['model_dof'].min()<15:\n",
    "        stock_sub = stock_sub[ stock_sub['model_dof']>15 ]\n",
    "\n",
    "    \n",
    "    # We now take the smallest chisq / ndof\n",
    "    try:\n",
    "        stock_sub = stock_sub[ stock_sub['chi2pdf']==stock_sub['chi2pdf'].min() ]\n",
    "    except KeyError:\n",
    "        print(stock_sub)\n",
    "        raise\n",
    "        break\n",
    "\n",
    "    # Just test this\n",
    "    parsnip_unique.append( dict(stock_sub.iloc[0]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc62d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsnip = pd.DataFrame.from_dict(parsnip_unique)\n",
    "print(df_parsnip.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a7615",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum( df_parsnip.duplicated('stock') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ce730",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ampelz = df_ampelz.merge(df_parsnip, how='outer', on='stock', suffixes=(None,'pn'))\n",
    "df_btsz = df_btsz.merge(df_parsnip, how='left', on='stock', suffixes=(None,'pn'))  # No point in adding all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bb5e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ampelz[ df_ampelz['stock']==304843764 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110993a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum( df_ampelz.duplicated('stock') )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d6cd23",
   "metadata": {},
   "source": [
    "Lets make an intermession and look for matches to the old pan void catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264c0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the CatalogQuery object pointing it to an existsing database\n",
    "mqc_query = CatalogQuery.CatalogQuery(\n",
    "        cat_name = 'voidGalPan',           # name of the database\n",
    "        coll_name = 'srcs',               # name of the collection with the sources\n",
    "        ra_key = 'ra', dec_key = 'dec',   # name of catalog fields for the coordinates\n",
    "        dbclient = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de57c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502775ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppcol = db.t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ea8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ampelz['z_void'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e92cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in df_ampelz.iterrows():\n",
    "    pp = ppcol.find_one({'stock':doc['stock'], 'id':{\"$gte\":0}})\n",
    "    ra = pp['body']['ra']\n",
    "    dec = pp['body']['dec']\n",
    "    hpcp, hpcp_dist = mqc_query.findclosest(ra, dec, 10, method = 'healpix')\n",
    "    \n",
    "    if hpcp is None:\n",
    "        continue\n",
    "\n",
    "    print(i)\n",
    "    print(doc['stock'])\n",
    "    print(hpcp)\n",
    "    print(hpcp_dist)\n",
    "    df_ampelz['z_void'].iloc[i] = hpcp['z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ampelz[df_ampelz['z_void']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let us save for temporary inspection\n",
    "df_ampelz.to_csv('/home/jnordin/tmp/allIa_ampelz.csv')\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btsz.to_csv('/home/jnordin/tmp/allIa_btsz.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c537f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which configurations correspond to what setting:\n",
    "# 3077100070068507562    - AmpelZ with max group 7\n",
    "# -7755784680569905391   - AmpelZ with max group 3. This can be IGNORED as its a subset of above.\n",
    "# 7231927385063365296    - BTS z\n",
    "# 6841134267660678928    - Seems to be the same as 3077100070068507562 but that this does not plot\n",
    "# -5764621775949025619   - Same as 7231927385063365296 but without plots\n",
    "# Also add Parsnip while we are here:\n",
    "# 1079222371588722561    - Parsnip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9aa8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_saved_saltfits(config, min_dof):\n",
    "    \"\"\"\n",
    "    Go through all sncosmo files with this config, a min degree of freedom and a converged fit.\n",
    "    \n",
    "    Return fit metrics and results.\n",
    "    \"\"\"\n",
    "    \n",
    "    stored = []\n",
    "    \n",
    "    for t2info in col.find({\"unit\":\"T2RunSncosmo\", \"config\": config, \n",
    "                            \"body.sncosmo_result.ndof\":{ \"$gte\": min_dof},\n",
    "                            \"body.sncosmo_result.success\": True}):\n",
    "        bodies = [b for b in t2info['body'] if (\n",
    "                    'sncosmo_result' in b.keys() \n",
    "                    and b['sncosmo_result']['success']\n",
    "                    and 'paramdict' in b['sncosmo_result'].keys()\n",
    "                    and b['sncosmo_result']['paramdict']['mwr_v']==3.1 )\n",
    "               ]\n",
    "        if len(bodies)==0:\n",
    "            # Worked with free mw, not w/o? poor lc\n",
    "            print('no success with fix mw?')\n",
    "            continue\n",
    "        body = bodies[-1]\n",
    "        if not body['sncosmo_result']['success']:\n",
    "            \n",
    "            print('success then failure?')\n",
    "            print(t2info)\n",
    "            print(body)\n",
    "            sys.exit('fix!')\n",
    "        tostore = { k:float(v) for k, v in body['sncosmo_result'].items() if k in \n",
    "                 ['chisq','ndof'] }\n",
    "        tostore['stock'] = t2info['stock']\n",
    "        tostore.update( body['sncosmo_result']['paramdict'])\n",
    "        tostore.update( {'{}_err'.format(k):v for k,v in body['sncosmo_result']['errors'].items() } )\n",
    "        tostore.update( body['fit_metrics'])\n",
    "\n",
    "        \n",
    "    \n",
    "        stored.append(tostore)\n",
    "        if len(stored)%10000==0:\n",
    "            print(len(stored))\n",
    "            \n",
    "    return stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b39187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_saved_parsnip(config, min_dof):\n",
    "    \"\"\"\n",
    "    Go through all parsnip files with this config, a min degree of freedom and a converged fit.\n",
    "    \n",
    "    Return fit metrics and results.\n",
    "    \"\"\"\n",
    "    \n",
    "    # What to store\n",
    "    stored = []\n",
    "    \n",
    "    for t2info in col.find({\"unit\":\"T2RunParsnip\", \"config\": config, \n",
    "                            \"body.prediction.model_dof\":{ \"$gte\": min_dof}}):\n",
    "        body = t2info['body'][-1] \n",
    "        tostore = { k:float(v) for k, v in body['prediction'].items() if k not in \n",
    "                 ['ra','dec', 'type', ] }\n",
    "        tostore['stock'] = t2info['stock']\n",
    "        tostore.update( body['classification'] )\n",
    "    \n",
    "        stored.append(tostore)\n",
    "        if len(stored)%10000==0:\n",
    "            print(len(stored))\n",
    "            \n",
    "    return stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf31a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "is1 = 0.3\n",
    "is2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb517ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is2 is None:\n",
    "    print('foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e967756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc7a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = 'ztf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a70d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all = 'baldsflkjasfaua/ztf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if re.search(sub, all ):\n",
    "    print('finding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b5a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
